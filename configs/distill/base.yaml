model:
  encoder_path: "checkpoints/base_20251228_145048/final"
  freeze_encoder: false
  encoder_lr_scale: 0.1

  # Projector bottleneck dimension (configurable 26-128)
  projector_dim: 128

  # Temporal adapter (matches encodercnn deployment)
  adapter_num_stages: 2
  adapter_kernel_size: 5
  adapter_stride: 2

  # CTC decoder (matches mobile target)
  rnn_type: lstm
  rnn_hidden: 128
  rnn_layers: 1
  rnn_bidirectional: true
  rnn_dropout: 0.1

  num_chars: 26
  blank_idx: 26

  # Text masking: 1.0 = full modality mode (forces path to carry all info)
  text_mask_prob: 1.0

data:
  dataset_name: "futo-org/swipe.futo.org"
  train_split: "train"
  val_split: "validation"
  path_resample_mode: "time"

  # Extra NPZ datasets (path features + words)
  extra_npz_paths:
    - "/home/lee/Documents/FUTO/encodercnn/data/npz/hws/"
    - "/home/lee/Documents/FUTO/encodercnn/data/npz/staticwebtests_vocab_coverage_20260212/"

  max_train_samples: null
  max_eval_samples: 10000

training:
  training_args:
    output_dir: "checkpoints/distill"
    logging_dir: "logs/distill"
    num_train_epochs: 10
    per_device_train_batch_size: 128
    per_device_eval_batch_size: 256
    gradient_accumulation_steps: 2
    learning_rate: 0.0001
    weight_decay: 0.01
    warmup_ratio: 0.05
    lr_scheduler_type: "cosine"
    logging_steps: 50
    eval_strategy: "steps"
    eval_steps: 2000
    save_strategy: "steps"
    save_steps: 2000
    save_total_limit: 3
    load_best_model_at_end: true
    metric_for_best_model: "eval_loss"
    greater_is_better: false
    bf16: true
    fp16: false
    remove_unused_columns: false
    report_to:
      - "tensorboard"
    dataloader_num_workers: 4
    dataloader_pin_memory: true
    max_grad_norm: 1.0
    save_safetensors: true
